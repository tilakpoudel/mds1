# Connect to the running Docker container
# remDr <- remoteDriver(
#   remoteServerAddr = "localhost",
#   port = 4444L,
#   browserName = "chrome"
#   extraCapabilities = chrome_capabilities
# )
# Open browser
remDr$open()
# Navigate and interact
remDr$navigate("https://www.google.com")
Sys.sleep(3)
# Get title to ensure connection works
page_title <- remDr$getTitle()
print(page_title)
library(RSelenium)
# Create a remote driver object
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444L,
browserName = "firefox"  # Specify Firefox browser
)
library(RSelenium)
# Create a remote driver object
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444L,
browserName = "firefox"  # Specify Firefox browser
)
# Open the browser
remDr$open()
# Navigate to a website
url <- "https://www.google.com"
remDr$navigate(url)
# Wait for the page to load
Sys.sleep(3)
# Get the title of the page
page_title <- remDr$getTitle()
print(page_title)
# Create a remote driver object with more defined capabilities
remDr <- remoteDriver(
remoteServerAddr = "localhost",   # Ensure this is correct (use container IP if necessary)
port = 4444L,
browserName = "firefox",          # Ensure 'firefox' is specified as the browser
extraCapabilities = list(
"moz:firefoxOptions" = list(
args = list("--headless")     # If you want headless mode, keep this, else remove it
)
)
)
# Open the browser
remDr$open()
# Navigate to a website
url <- "https://www.google.com"
remDr$navigate(url)
# Wait for the page to load
Sys.sleep(3)
# Get the title of the page
page_title <- remDr$getTitle()
print(page_title)
# Extract the page source (for further scraping)
page_source <- remDr$getPageSource()
print(page_source)
# Create a remote driver object with more defined capabilities
remDr <- remoteDriver(
remoteServerAddr = "172.18.0.2",   # Ensure this is correct (use container IP if necessary)
port = 4444L,
browserName = "firefox",          # Ensure 'firefox' is specified as the browser
extraCapabilities = list(
"moz:firefoxOptions" = list(
args = list("--headless")     # If you want headless mode, keep this, else remove it
)
)
)
# Open the browser
remDr$open()
# Navigate to a website
url <- "https://www.google.com"
remDr$navigate(url)
# Wait for the page to load
Sys.sleep(3)
# Get the title of the page
page_title <- remDr$getTitle()
print(page_title)
# Extract the page source (for further scraping)
page_source <- remDr$getPageSource()
print(page_source)
library(RSelenium)
library(RSelenium)
remDr <- remoteDriver(
remoteServerAddr = "localhost",
port = 4444L,
browserName = "chrome"  # Selenium will automatically choose Chromium if it's available
)
remDr$open()
if (is.na(remDr$sessionInfo$id)) {
stop("Failed to create a session. Check Selenium server logs.")
}
library(httr)
library(jsonlite)
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# API URL
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to the API
response <- GET(url, query = list())
# Check if the response was successful (status code 200)
if (status_code(response) == 200) {
# Parse the response content
data <- content(response, "text")
parsed_data <- fromJSON(data)
print(parsed_data)
# Get covid case counts of india from api and save in json file
library(httr)
library(jsonlite)
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# API URL
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to the API
response <- GET(url, query = list())
# Check if the response was successful (status code 200)
if (status_code(response) == 200) {
# Parse the response content
data <- content(response, "text")
parsed_data <- fromJSON(data)
print(parsed_data)
# Save the data to a file (JSON format)
output_file <- "output/india_covid_cases_count_data.json"
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data saved to", output_file, "\n")
} else {
cat("Error: Unable to fetch data. Status code:", status_code(response), "\n")
}
# Make the GET request to the API
response <- GET(url, query = list())
print(response)
# Get covid case counts of india from api and save in json file
library(httr)
library(jsonlite)
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# API URL
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to the API
response <- GET(url, query = list())
print(response)
# Check if the response was successful (status code 200)
if (status_code(response) == 200) {
# Parse the response content
data <- content(response, "text")
parsed_data <- fromJSON(data)
print(parsed_data)
# Save the data to a file (JSON format)
output_file <- "output/india_covid_cases_count_data.json"
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data saved to", output_file, "\n")
} else {
cat("Error: Unable to fetch data. Status code:", status_code(response), "\n")
}
response <- GET(url)
print(response)
# Load necessary libraries
library(httr)
library(jsonlite)
url <- "https://api.rootnet.in/covid19-in/stats/latest"
response <- GET(url)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Print a preview of the data (optional)
print(parsed_data)
# Load necessary libraries
library(httr)
library(jsonlite)
# Define the API endpoint
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to fetch data
response <- GET(url)
# Check if the request was successful (HTTP status 200)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Print a preview of the data (optional)
print(parsed_data)
# Define output file name
output_file <- "covid19_in_latest.json"
# Save the parsed data into a JSON file (formatted nicely)
write_json(parsed_data, output_file, pretty = TRUE)
cat("✅ Data successfully saved to:", output_file, "\n")
} else {
# If the request fails, print an error message
cat("❌ Error: Failed to fetch data. Status code:", status_code(response), "\n")
}
library(httr)
library(jsonlite)
library(httr)
library(jsonlite)
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# API URL
url <- "https://api2.waqi.info/api/feed/@10493/aqi.json"
# Make the GET request to the API
response <- GET(url, query = list())
# Check if the response was successful (status code 200)
if (status_code(response) == 200) {
# Parse the response content
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Save the data to a file (JSON format)
output_file <- "output/kathmandu_aqi_data.json"
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data saved to", output_file, "\n")
} else {
cat("Error: Unable to fetch data. Status code:", status_code(response), "\n")
}
print(3)
print(3+2)
library(httr)
library(jsonlite)
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# API URL
url <- "https://api2.waqi.info/api/feed/@10493/aqi.json"
# Make the GET request to the API
response <- GET(url, query = list())
# Check if the response was successful (status code 200)
if (status_code(response) == 200) {
# Parse the response content
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Save the data to a file (JSON format)
output_file <- "output/kathmandu_aqi_data.json"
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data saved to", output_file, "\n")
} else {
cat("Error: Unable to fetch data. Status code:", status_code(response), "\n")
}
# Load necessary libraries
library(httr)
library(jsonlite)
# Define the API endpoint
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to fetch data
response <- GET(url)
# Check if the request was successful (HTTP status 200)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Print a preview of the data (optional)
print(parsed_data)
# Define output file name
output_file <- "covid19_in_latest.json"
# Save the parsed data into a JSON file (formatted nicely)
write_json(parsed_data, output_file, pretty = TRUE)
cat("✅ Data successfully saved to:", output_file, "\n")
} else {
# If the request fails, print an error message
cat("❌ Error: Failed to fetch data. Status code:", status_code(response), "\n")
}
# Load necessary libraries
library(httr)
library(jsonlite)
setwd("~/projects/tilak/mds1/R-programming")
# Define the API endpoint
url <- "https://api.rootnet.in/covid19-in/stats/latest"
# Make the GET request to fetch data
response <- GET(url)
# Check if the request was successful (HTTP status 200)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Print a preview of the data (optional)
print(parsed_data)
# Define output file name
output_file <- "output/covid19_in_latest.json"
# Save the parsed data into a JSON file (formatted nicely)
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data successfully saved to:", output_file, "\n")
} else {
# If the request fails, print an error message
cat("Error: Failed to fetch data. Status code:", status_code(response), "\n")
}
library(httr)
library(jsonlite)
setwd("~/projects/tilak/mds1/R-programming")
# Define the API endpoint
url <- "https://api.rootnet.in/covid19-in/stats/history"
# Make the GET request to fetch data
response <- GET(url)
# Check if the request was successful (HTTP status 200)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
library(httr)
library(jsonlite)
setwd("~/projects/tilak/mds1/R-programming")
# Define the API endpoint
url <- "https://api.rootnet.in/covid19-in/stats/history"
# Make the GET request to fetch data
response <- GET(url)
# Check if the request was successful (HTTP status 200)
if (status_code(response) == 200) {
# Parse the JSON content from the response
data <- content(response, "text")
parsed_data <- fromJSON(data)
# Define output file name
output_file <- "output/covid19_in_history.json"
# Save the parsed data into a JSON file (formatted nicely)
write_json(parsed_data, output_file, pretty = TRUE)
cat("Data successfully saved to:", output_file, "\n")
} else {
# If the request fails, print an error message
cat("Error: Failed to fetch data. Status code:", status_code(response), "\n")
}
library(rvest)
library(jsonlite)
# set the working directory
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# web page url
url = "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
webpage <- read_html(url)
# observe the structure of the web page
str(webpage)
library(rvest)
library(dplyr)
library(stringr)
# Load the webpage
url <- "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
webpage <- read_html(url)
get_table_by_caption <- function(page, caption_text) {
page %>%
html_nodes("table") %>%
keep(~ html_node(.x, "caption") %>% html_text() %>% str_detect(caption_text)) %>%
.[[1]] %>%
html_table(fill = TRUE)
}
deaths_table <- get_table_by_caption(webpage, "2022 monthly cumulative COVID-19 deaths")
deaths_table <- get_table_by_caption(webpage, "2022 monthly cumulative COVID-19 deaths")
# Define the Wikipedia URL
url <- "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
webpage <- read_html(url)
covid_deaths_table <- webpage %>%
html_node(xpath = "//table[caption[contains(text(), '2022 monthly cumulative COVID-19 deaths')]]") %>%
html_table(fill = TRUE)
if (is.null(covid_deaths_table)) {
stop("⚠️ Table not found!")
} else {
print(head(covid_deaths_table))
}
covid_vaccine_table <- webpage %>%
html_node("table.wikitable:has(caption:contains('COVID-19 vaccine distribution by country'))") %>%
html_table(fill = TRUE)
# use class and caption to get the desired table
covid_vaccine_table <- webpage %>%
html_node("table.wikitable:has(`caption:contains('COVID-19 vaccine distribution by country')`)") %>%
html_table(fill = TRUE)
covid_vaccine_table <- webpage %>%
html_node("table.wikitable:has(caption:contains('COVID-19 vaccine distribution by country'))") %>%
html_table(fill = TRUE)
get_table_by_caption <- function(page, caption_text) {
page %>%
html_node(xpath = paste0("//table[caption[contains(text(), '", caption_text, "')]]")) %>%
html_table(fill = TRUE)
}
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution by country")
get_table_by_caption <- function(page, caption_text) {
table_node <- page %>%
html_node(xpath = paste0("//table[caption[contains(., '", caption_text, "')]]"))
if (is.null(table_node)) {
stop(paste("⚠️ Table with caption containing '", caption_text, "' not found!", sep = ""))
}
return(html_table(table_node, fill = TRUE))
}
# Test the vaccine table again
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution")
print(head(vaccine_table))
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution by country")
print(head(vaccine_table))
vaccine_table1 <- get_table_by_caption(webpage, "COVID-19 vaccine distribution by country")
print(head(vaccine_table1))
vaccine_table1 <- get_table_by_caption(webpage, "COVID-19 cases, deaths, and rates by location")
print(head(vaccine_table1))
death_and_rates_by_location <- get_table_by_caption(webpage, "COVID-19 cases, deaths, and rates by location")
print(head(death_and_rates_by_location))
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution")
# print(head(vaccine_table))
# Save the data as csv
write.csv(vaccine_table, "output/vaccine_distribution_table.csv", row.names = FALSE)
get_table_by_caption <- function(page, caption_text) {
# Step 1: Find the table by caption using XPath
# paste0(): Since we don’t need any spaces between the parts of the XPath query,
# it is appropriate because it directly concatenates the strings without adding a space.
table_node <- page %>%
html_node(xpath = paste0("//table[caption[contains(., '", caption_text, "')]]"))
if (is.null(table_node)) {
stop(paste("Table with caption containing '", caption_text, "' not found!", sep = ""))
}
# Step 2: Convert table node to a data frame
table_df <- html_table(table_node, fill = TRUE)
# Step 3: Skip the first column and last row
table_df <- table_df[, -1]   # Remove the first column
table_df <- table_df[-nrow(table_df), ]  # Remove the last row
return(table_df)
}
# Get the vaccine table
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution")
print(head(vaccine_table))
library(rvest)
library(jsonlite)
# set the working directory
getwd()
setwd("~/projects/tilak/mds1/R-programming")
# Define the Wikipedia URL
url <- "https://en.wikipedia.org/wiki/COVID-19_pandemic_by_country_and_territory"
webpage <- read_html(url)
# Function to get the table by xpath and caption and modify it (skip first column, last row)
get_table_by_caption <- function(page, caption_text) {
# Step 1: Find the table by caption using XPath
# paste0(): Since we don’t need any spaces between the parts of the XPath query,
# it is appropriate because it directly concatenates the strings without adding a space.
table_node <- page %>%
html_node(xpath = paste0("//table[caption[contains(., '", caption_text, "')]]"))
if (is.null(table_node)) {
stop(paste("Table with caption containing '", caption_text, "' not found!", sep = ""))
}
# Step 2: Convert table node to a data frame
table_df <- html_table(table_node, fill = TRUE)
# Step 3: Skip the first column and last row
table_df <- table_df[, -1]   # Remove the first column
table_df <- table_df[-nrow(table_df), ]  # Remove the last row
return(table_df)
}
# Get the vaccine table
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution")
print(head(vaccine_table))
# Save the data as csv
write.csv(vaccine_table, "output/vaccine_distribution_table.csv", row.names = FALSE)
death_and_rates_by_location <- get_table_by_caption(webpage, "COVID-19 cases, deaths, and rates by location")
print(head(death_and_rates_by_location))
write.csv(death_and_rates_by_location, "output/deaths_table.csv", row.names = FALSE)
cat("Tables scraped and saved!")
death_and_rates_by_location <- get_table_by_caption(webpage, "COVID-19 cases, deaths, and rates by location")
print(head(death_and_rates_by_location))
write.csv(death_and_rates_by_location, "output/deaths_table.csv", row.names = FALSE)
get_table_by_caption <- function(page, caption_text, trimFirstColumn = FALSE, trimLastRow = FALSE) {
# Step 1: Find the table by caption using XPath
# paste0(): Since we don’t need any spaces between the parts of the XPath query,
# it is appropriate because it directly concatenates the strings without adding a space.
table_node <- page %>%
html_node(xpath = paste0("//table[caption[contains(., '", caption_text, "')]]"))
if (is.null(table_node)) {
stop(paste("Table with caption containing '", caption_text, "' not found!", sep = ""))
}
# Step 2: Convert table node to a data frame
table_df <- html_table(table_node, fill = TRUE)
# Step 3: Skip the first column and last row
if (trimFirstColumn) {
table_df <- table_df[, -1]   # Remove the first column
}
if (trimLastRow) {
table_df <- table_df[-nrow(table_df), ]  # Remove the last row
}
return(table_df)
}
vaccine_table <- get_table_by_caption(webpage, "COVID-19 vaccine distribution", trimFirstColumn = TRUE, trimLastRow = TRUE)
print(head(vaccine_table))
death_and_rates_by_location <- get_table_by_caption(webpage, "COVID-19 cases, deaths, and rates by location", trimLastRow = TRUE)
print(head(death_and_rates_by_location))
monthly_2022_cumm_deaths <- get_table_by_caption(webpage, captionText = "2022 monthly cumulative COVID-19 deaths", trimFirstColumn = TRUE)
# Function to get the table by xpath and caption and modify it (skip first column, last row)
get_table_by_caption <- function(page, captionText, trimFirstColumn = FALSE, trimLastRow = FALSE) {
# Step 1: Find the table by caption using XPath
# paste0(): Since we don’t need any spaces between the parts of the XPath query,
# it is appropriate because it directly concatenates the strings without adding a space.
table_node <- page %>%
html_node(xpath = paste0("//table[caption[contains(., '", captionText, "')]]"))
if (is.null(table_node)) {
stop(paste("Table with caption containing '", captionText, "' not found!", sep = ""))
}
# Step 2: Convert table node to a data frame
table_df <- html_table(table_node, fill = TRUE)
# Step 3: Skip the first column and last row
if (trimFirstColumn) {
table_df <- table_df[, -1]   # Remove the first column
}
if (trimLastRow) {
table_df <- table_df[-nrow(table_df), ]  # Remove the last row
}
return(table_df)
}
# Table3
monthly_2022_cumm_deaths <- get_table_by_caption(webpage, captionText = "2022 monthly cumulative COVID-19 deaths", trimFirstColumn = TRUE)
print(head(monthly_2022_cumm_deaths))
monthly_2022_cumm_deaths <- get_table_by_caption(webpage, captionText = "2022 monthly cumulative COVID-19 deaths", trimFirstColumn = TRUE)
print(head(monthly_2022_cumm_deaths))
# Save the data as csv
write.csv(monthly_2022_cumm_deaths, "output/monthly_2022_cumm_deaths", row.names = FALSE)
monthly_2022_cumm_deaths <- get_table_by_caption(webpage, captionText = "2022 monthly cumulative COVID-19 deaths", trimFirstColumn = TRUE)
print(head(monthly_2022_cumm_deaths))
# Save the data as csv
write.csv(monthly_2022_cumm_deaths, "output/monthly_2022_cumm_deaths.csv", row.names = FALSE)
monthly_2022_cumm_deaths <- get_table_by_caption(webpage, captionText = "2022 monthly cumulative COVID-19 deaths", trimFirstColumn = TRUE)
print(head(monthly_2022_cumm_deaths))
# Save the data as csv
write.csv(monthly_2022_cumm_deaths, "output/monthly_2022_cumm_deaths.csv", row.names = FALSE)
