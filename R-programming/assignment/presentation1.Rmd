---
title: "39_Presentation1"
author: "Tilak Poudel"
date: "2025-05-04"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# PRESENTATION 1

# Supervised Learning
## 1. Divide "mtcars" dataset as training data (70% random cases) and testing data (30% random cases) using "sample" in r
## 2. Fit simple linear regression models on training data with mpg as dependent and all other variables as independent variables one by one i.e. separately. Are these models BLUE? Why?
## 3. Identify the statistically significant (p<0.05) independent variables from simple linear regression models as potential candidate variables for the final model and list them for next step
## 4. Fit a multiple linear regression model on training data with mpg as dependent and all the statistically significant variables from simple linear regression models
## 5. Get VIF of all these variables to check multicollinearity and run the final model until none of the variables have VIF >= 10
## 6. Get summary and accuracy indices (R-square, RMSE, MAE) of the final model fitted with variables having VIF < 10
## 7. Use lasso regularization as alternative to deal with multicollinearity, show the results in the PPT and explain them well
## 8. Perform residual analysis on the final model using LINE tests. Can you do prediction using this model? Why?
## 9. Predict the mpg on testing data, get accuracy indices (R-square, RMSE, MAE) of prediction and interpret them carefully
## 10. Prediction: How much mpg is given by a car with 6000 lbs weight based on training and testing data? Which is correct?
## 11. Write a summary based on the results obtained above and include recommendations using data science approach.



# Load the libraries
```{r}
library(caret)
library(car)
library(lmtest)
library(glmnet)
```

# 1. Load the data and partition it
```{r load_data}
data = mtcars
str(data)
names(data)
# Set seed for reproducibility
set.seed(39)
# Data partition
ind <- sample(2, nrow(data), replace = T,prob = c(0.7, 0.3))
print(ind)

# Training data
train_data <- data[ind == 1,]
train_data
# Check the partition
table(ind)

# Test data
test_data <- data[ind == 2,]
test_data
# Check the partition
table(ind)

```

# 2. Fit simple linear regression models on training data with mpg as dependent and all other variables as independent variables one by one i.e. separately. Are these models BLUE? Why?
```{r simple_regression}
# Fit simple linear regression models
lm1 <- lm(mpg ~ cyl, data = train_data)
summary(lm1)
```
The p-value(1.782e-07) is less than 0.05, indicating that the model is statistically significant. The R-squared value is 0.7013 (>0.5), which means that 70.13% of the variance in mpg can be explained by the number of cylinders (cyl). The model is BLUE (Best Linear Unbiased Estimator).

```{r simple_regression2}
lm2 <- lm(mpg ~ disp, data = train_data)
summary(lm2)
```
The p-value(1.405e-07) of the model is less than 0.05, indicating that the model is statistically significant. The R-squared value is 0.7073 (>0.5), which means that 70.73% of the variance in mpg can be explained by the displacement (disp). The model is BLUE (Best Linear Unbiased Estimator).

```{r simple_regression3}
lm3 <- lm(mpg ~ hp, data = train_data)
summary(lm3)
```
The p-value(1.482e-05) of the model is less than 0.05, indicating that the model is statistically significant. The R-squared value is 0.565 (>0.5), which means that 56.5% of the variance in mpg can be explained by the horsepower (hp). The model is BLUE (Best Linear Unbiased Estimator).

```{r simple_regression4}
lm4 <- lm(mpg ~ drat, data = train_data)
summary(lm4)
```
The p-value(0.0001) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.3586 (<0.5), which means that only 35.86% of the variance in mpg can be explained by the rear axle ratio (drat). The model with drat is not  BLUE (Best Linear Unbiased Estimator).

```{r simple_regression5}
lm5 <- lm(mpg ~ wt, data = train_data)
summary(lm5)
```
The p-value(1.778e-08) of the model is less than 0.05, indicating that the model is statistically significant. The R-squared value is 0.7546 (>0.5), which means that 75.46% of the variance in mpg can be explained by the weight (wt). The model is BLUE (Best Linear Unbiased Estimator).

```{r simple_regression6}
lm6 <- lm(mpg ~ qsec, data = train_data)
summary(lm6)
```
The p-value(0.02328) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.2044 (<0.5), which means that only 20.44% of the variance in mpg can be explained by the quarter mile time (qsec). The model with qsec is not BLUE (Best Linear Unbiased Estimator).

```{r simple_regression7}
lm7 <- lm(mpg ~ vs, data = train_data)
summary(lm7)
```
The p-value(0.0007203) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.3981 (<0.5), which means that only 39.81% of the variance in mpg can be explained by the engine type (vs). The model with vs is not BLUE (Best Linear Unbiased Estimator).

```{r simple_regression8}
lm8 <- lm(mpg ~ am, data = train_data)
summary(lm8)
```
The p-value(0.009233) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.2599 (<0.5), which means that only 25.99% of the variance in mpg can be explained by the transmission type (am). The model with am is not BLUE (Best Linear Unbiased Estimator).

```{r simple_regression9}
lm9 <- lm(mpg ~ gear, data = train_data)
summary(lm9)
```
The p-value(0.03793) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.1742 (<0.5), which means that only 17.42% of the variance in mpg can be explained by the number of forward gears (gear). The model with gear is not BLUE (Best Linear Unbiased Estimator).

```{r simple_regression10}
lm10 <- lm(mpg ~ carb, data = train_data)
summary(lm10)
```
The p-value(0.002849) of the model is less than 0.05, indicating that the model is statistically significant But the R-squared value is 0.3265 (<0.5), which means that only 32.65% of the variance in mpg can be explained by the number of carburetors (carb). The model with carb is not BLUE (Best Linear Unbiased Estimator).

# 3. Identify the statistically significant (p<0.05) independent variables from simple linear regression models as potential candidate variables for the final model and list them for next step
```{r significant_variables}
results <- lapply(names(data)[-1], function(var) {
  formula <- as.formula(paste("mpg ~", var))
  model <- lm(formula, data = train_data)
  summary(model)$coefficients[2, 4]  # p-value of the predictor
})
print(results)
names(results) <- names(data)[-1]
print(names(results))
sig_vars <- names(results)[unlist(results) < 0.05]
sig_vars
```

The variables with p-value <0.05 and R-squared value greater than 0.5 are significant and can be used as potential candidate variables for the final model. The significant variables are:
- cyl
- disp
- hp
- wt

# 4. Fit a multiple linear regression model on training data with mpg as dependent and all the statistically significant variables from simple linear regression models
```{r multiple_regression}
mlr <- lm(mpg ~ cyl + disp + hp + wt, data = train_data)
summary(mlr)
```
The p-value(5.855e-08) of the model is less than 0.05, indicating that the model is statistically significant. The R-squared value is 0.849 (>0.5), which means that 84.9% of the variance in mpg can be explained by the number of cylinders (cyl), displacement (disp), horsepower (hp), and weight (wt).

# 5. Get VIF of all these variables to check multicollinearity and run the final model until none of the variables have VIF >= 10

```{r vif_linear_model}
# multiple linear model with all variables
mlr0 <- lm(mpg ~ ., data = train_data)
summary(mlr0)
# Check VIF
vif(mlr0)
```
The VIF values for the variables are greater than 10, indicating that there is multicollinearity among the variables. The variables with VIF > 10 are : cycl, disp, wt. So remove them

```{r remove_multicollinearity}
# Remove the variables with VIF > 10
mlr2 <- lm(mpg ~ drat+wt+qsec+vs+am+gear+carb, data = train_data)
vif(mlr2)
summary(mlr2)
```

```{r vif}
vif(mlr)

```
ALthought the VIF for other variable are <=10. From step 3 we got the significiant variables with R-squared > 0.5. So the variables with R-squared >0.5 and vif <=10 are considered statistically significiant. The VIF values for the possible statistically significant variables are less than 10, indicating that there is no multicollinearity among the variables. So the variables `cyl, disp, hp, and wt` can be used in the final model.

# 6. Get summary and accuracy indices (R-square, RMSE, MAE) of the final model fitted with variables having VIF < 10
```{r accuracy_indices}
# Calculate RMSE
rmse <- sqrt(mean(residuals(mlr)^2))
# Calculate MAE
mae <- mean(abs(residuals(mlr)))
# Calculate R-squared
r_squared <- summary(mlr)$r.squared
# Print the accuracy indices
cat("R-squared:", r_squared, "\t", "RMSE:", rmse, "\t", "MAE:", mae, "\n")
```
The R-squared value is 0.849, RMSE is 2.55, and MAE is 2.05. The R-squared value indicates that 84.9% of the variance in mpg can be explained by the number of cylinders (cyl), displacement (disp), horsepower (hp), and weight (wt). The RMSE and MAE values indicate that the model has a good fit to the data.

# 7. Use lasso regularization as alternative to deal with multicollinearity, show the results in the PPT and explain them well
```{r lasso}
# Prepare the data for lasso regression
x_train <- model.matrix(mpg ~ ., data = train_data)[, -1]
y_train <- train_data$mpg
x_test <- model.matrix(mpg ~ ., data = test_data)[, -1]
y_test <- test_data$mpg
# Fit lasso regression
lasso_model <- glmnet(x_train, y_train, alpha = 1)
summary(lasso_model)
plot(lasso_model, xvar = "lambda", label = TRUE)
```
The lasso regression model is fitted using the glmnet package. The plot shows the coefficients of the variables as a function of the regularization parameter (lambda). As lambda increases, the coefficients shrink towards zero, indicating that some variables are less important than others. The lasso regression can help to reduce multicollinearity by selecting only the most important variables.

```{r lasso_cv}
# Cross-validation for lasso regression
lasso_cv <- cv.glmnet(x_train, y_train, alpha = 1)
plot(lasso_cv)

# Get the best lambda value
best_lambda <- lasso_cv$lambda.min
print(best_lambda)

# Step 4: Extract coefficients at best lambda
coef_lasso <- coef(lasso_cv, s = "lambda.min")
coef_lasso

# See selected variables
selected_vars <- rownames(coef_lasso)[which(coef_lasso != 0)]
print(selected_vars)

```
From the cross-validation plot, we can see that the best lambda value is 0.6418. The coefficients at the best lambda value indicate that the variables with non-zero coefficients are selected by the lasso regression. The selected variables are: cyl, hp and wt, qsec and vs. The lasso regression has selected only the most important variables and reduced the multicollinearity among the variables.

# 8. Perform residual analysis on the final model using LINE tests. Can you do prediction using this model? Why?
```{r residual_analysis}
# Residual analysis
par(mfrow = c(2, 2))
plot(mlr)

```
## 1. Lineraity test
### Graphical test
```{r linearity_test_graphical}
# Graphical test
plot(mlr, which = 1)
```

Visual inspection of the residuals plot shows that the residuals are randomly scattered around zero, indicating that the linearity assumption is satisfied. The residuals are normally distributed and have constant variance. The residuals are also independent of the fitted values.

### Statistical test
```{r linearity_test_statistical}
# Statistical test
summary(mlr$residuals)
```
The mean of the residuals is zero, indicating that the model is unbiased. The residuals are normally distributed and have constant variance. The residuals are also independent of the fitted values.

## 2. Independence of residuals test

### Graphical test
```{r independence_test_graphical}
# Graphical test(suggestive)
acf(mlr$residuals)
```
The acf plot shows that the residuals are not correlated with each other, indicating that the independence assumption is satisfied. The residuals are also normally distributed and have constant variance. The residuals are also independent of the fitted values.

### Statistical test
```{r independence_test_statistical}
# Statistical test
durbinWatsonTest(mlr)

```
The p-value of the Durbin-Watson test is 0.134, which is greater than 0.05, indicating that there is no autocorrelation in the residuals. The residuals are also normally distributed and have constant variance. The residuals are also independent of the fitted values.

## 3. Normality test
### Graphical test
```{r normality_test_graphical}
# Graphical test
hist(mlr$residuals)
```
The histogram of the residuals shows that the residuals are normally distributed, indicating that the normality assumption is satisfied. The residuals are also independent of the fitted values.

### Statistical test
```{r normality_test_statistical}
# Statistical test
shapiro.test(mlr$residuals)
```
The p-value of the Shapiro-Wilk test is 0.3197, which is greater than 0.05, indicating that the residuals are normally distributed. The residuals are also independent of the fitted values.


## 4. Equal variance test
### Graphical test
```{r equal_variance_test_graphical}
# Graphical test
plot(mlr, which = 3, col = c("blue"))
```
The scale-location plot shows that the residuals are randomly scattered, indicating that the equal variance((homoscedasticity)) assumption is satisfied.

### Statistical test
```{r equal_variance_test_statistical}
# Statistical test
bptest(mlr)
```
The p-value of the Breusch-Pagan test is 0.177, which is greater than 0.05, indicating that the equal variance assumption is satisfied.

## Conclusion
The residual analysis shows that the linearity, independence, normality, and equal variance assumptions are satisfied. Therefore, we can use this model for prediction. The model is BLUE (Best Linear Unbiased Estimator) and can be used for prediction.

# 9. Predict the mpg on testing data, get accuracy indices (R-square, RMSE, MAE) of prediction and interpret them carefully
```{r prediction}
# Predict the mpg on testing data
predictions <- predict(mlr, newdata = test_data)
# Calculate RMSE
rmse_test <- sqrt(mean((predictions - test_data$mpg)^2))
# Calculate MAE
mae_test <- mean(abs(predictions - test_data$mpg))
# Calculate R-squared
r_squared_test <- 1 - (sum((predictions - test_data$mpg)^2) / sum((mean(test_data$mpg) - test_data$mpg)^2))
# Print the accuracy indices
cat("R-squared:", r_squared_test, "\t", "RMSE:", rmse_test, "\t", "MAE:", mae_test, "\n")
```
The R-squared value is 0.7648606, RMSE is 3.1926, and MAE is 2.5295. The R-squared value indicates that 76.48% of the variance in mpg can be explained by the number of cylinders (cyl), displacement (disp), horsepower (hp), and weight (wt). The RMSE and MAE values indicate that the model has a good fit to the data.
RMSE 3.19 means taht the average difference between the predicted and actual values is 3.19 mpg. MAE 2.529 means that the average absolute difference between the predicted and actual values is 2.529 mpg. The model has a good fit to the data and can be used for prediction.

# 10. Prediction: How much mpg is given by a car with 6000 lbs weight based on training and testing data? Which is correct?
```{r prediction_6000lbs_train_data}
# Predict the mpg for a car with 6000 lbs weight based on test data
new_data <- data.frame(wt = 6, cyl = 6, disp = 200, hp = 100)
predicted_mpg_train <- predict(mlr, newdata = new_data)
predicted_mpg_train
```
The predicted mpg for a car with 6000 lbs weight is 8.5 mpg based on the training data. The predicted mpg for a car with 6000 lbs weight is 8.5 mpg based on the training data.
```{r prediction_6000lbs_trainig_data}
mlr2 <- lm(mpg ~ cyl + disp + hp + wt, data = test_data)
new_data_test <- data.frame(wt = 6, cyl = 6, disp = 200, hp = 100)
predicted_mpg_test <- predict(mlr2, newdata = new_data_test)
predicted_mpg_test
```
The predicted mpg for a car with 6000 lbs weight is -4.33 mpg based on the testing data. The predicted mpg for a car with 6000 lbs weight is 8.5 mpg based on the testing data.

The prediction of miles per gallon (8.51) for car with weight 6000lbs with training data is correct.

# 11. Write a summary based on the results obtained above and include recommendations using data science approach.
The analysis of the mtcars dataset using linear regression has shown that the model is statistically significant and can explain a large portion of the variance in mpg. The final model includes the variables cyl, disp, hp, and wt, which are statistically significant and have VIF values less than 10, indicating that there is no multicollinearity among the variables. The model has a good fit to the data, with R-squared value of 0.849, RMSE of 2.55, and MAE of 2.05.
The residual analysis shows that the linearity, independence, normality, and equal variance assumptions are satisfied. Therefore, we can use this model for prediction. The model is BLUE (Best Linear Unbiased Estimator) and can be used for prediction.
The lasso regression model has also been fitted to the data, which can help to reduce multicollinearity by selecting only the most important variables. The lasso regression has selected only the most important variables and reduced the multicollinearity among the variables.
