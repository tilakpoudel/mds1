---
title: "39-TilakPoudel-ClassPresentation"
author: "Tilak Poudel"
date: "2025-05-10"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

The slides must include a) code used to generate 500 random data with five variables b) code used to fix the random seed with roll number c) code used to fit the assigned supervised and unsupervised statistical models d) codes used to get model accuracy indices and e) interpretations of the outputs from these codes

Assigned models:

1. Roll 1 - 10: Fitting Logistic regression and KNN classification models and comparison of the model assumptions and accuracy indices to select the best model for the data based on training and testing datasets

2. Roll 11 - 20: Fitting Decision Tree and Random Forest models and comparison of the model assumptions and accuracy indices to select the best model for the data based on training and testing datasets

3. Roll 21 - 30: Fitting dimension (variables) reduction techniques i.e. PCA and MDS and selecting the best model for this data with careful interpretations of the bi-plots

4. Roll 31 - 40: Fitting dimension (cases) reduction techniques i.e. HCA and k-means and selecting the best model for this data with careful interpretations of the cluster plots

## Step (a): Generate 500 random observations with 5 variables
```{r data generation}
set.seed(39)

# Generate the data
data <- data.frame(
  X1 = rnorm(500, mean = 10, sd = 2),
  X2 = runif(500, min = 0, max = 100),
  X3 = rnorm(500, mean = 50, sd = 10),
  X4 = runif(500, min = 2, max=8),
  X5 = sample(1:3, 500, replace = TRUE)
)

# View first few rows
head(data)

print(dim(data))
```
# HCA (Hierarchical Cluster Analysis)
### Hiererchical clustering with single linkage

```{r hca}
# data <- scale(data[, 1:4])  # Use only independent variables

data.similarity <- dist(data)
hirar.1 <- hclust(data.similarity, method='single')
hirar.1

# Plot 
plot(
  hirar.1,
  labels=rownames(data),
  ylab="Distance"
)
```
### Hiererchical clustering with complete linkage
```{r}
hirar.2 <- hclust(data.similarity, method='complete')
hirar.2

# Plot 
plot(
  hirar.2,
  labels=rownames(data),
  ylab="Distance"
)

abline(h = 90, col = 'red')

```
### Hiererchical clustering with average linkage
```{r}
hirar.3 <- hclust(data.similarity, method='average')
hirar.3

# Plot 
plot(
  hirar.3,
  labels=rownames(data),
  ylab="Distance"
)
```
### Hiererchical clustering with centroid linkage
```{r}
hirar.4 <- hclust(data.similarity, method='centroid')
hirar.4

# Plot 
plot(
  hirar.4,
  labels=rownames(data),
  ylab="Distance"
)
```

## Selecting the number of clusters (k)
Observing the dendogram with various linkage, we can take similarity complete and cut at distance 90 to get 2 clusters. 

# Fit the model: Clustering(K-mean)
```{r model_fitting}
# Fit the k-mean clustering model on the data
# Scale if needed (optional here as all variables have same scale)
# data <- scale(data)

set.seed(39)
kmeans_model <- kmeans(data, centers=2, nstart = 20)
kmeans_model

# Check the clusters
#kmeans_model$cluster

# Add cluster labels
#data$kmeans_cluster <- factor(kmeans_model$cluster)
```
The data set are partitioned into 2 clusters.

Within-Cluster Sum of Squares (WSS). WSS measures how compact each cluster is.
Lower values means points are closer to their cluster center.
[1] 77062.14 84340.07
Cluster 2 is tighter (better fit) than the other
WSS values are fairly close, which seems to be good.

Between-Cluster vs Total Sum of Squares
   (between_SS / total_SS =  66.5 %)
  
- This is the proportion of variance explained by the clustering.

- 66.5% of the total variance is between clusters.

- Higher % (closer to 100%)specifies clusters are well separated.
 
## Plot the clusters
```{r plot clusters}
summary(data)
library(cluster)

# Reduce data to 2D with PCA for visualization.
# Since we have 5 variables it is not possible to visualize
pca_result <- prcomp(data, scale = FALSE)
pca_data <- pca_result$x[, 1:2]  # first two PCs

# Plot cluster
plot(
  pca_data, 
  col = kmeans_model$cluster, 
  main = "K-means Clustering (k = 2) on PCA-reduced Data", 
  xlab = "PC1", 
  ylab = "PC2", 
  pch = 20, 
  cex = 2
)

# Add cluster centers
points(
  aggregate(pca_data, by = list(kmeans_model$cluster), FUN = mean)[, 2:3],
  col = 1:3,
  pch = 8,
  cex = 3,
  lwd = 2
)

```

## Cusplot

```{r}
clusplot(
  data, 
  kmeans_model$cluster, 
  color = TRUE, 
  shade = TRUE, 
  labels = 2, 
  lines = 0, 
  main = "Clusplot of K-means Clustering (k = 2)"
)
```

## Evaluate with WSS and silhoutte
```{r evaluation}
# Silhouette
library(cluster)
sil <- silhouette(kmeans_model$cluster, dist(data))
mean(sil[, 3])  # Average silhouette width

```
